<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction">
  <meta property="og:title" content="Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction"/>
  <meta property="og:description" content="Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction"/>
  <meta property="og:url" content="https://example.com"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Open3DHOI">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Computer Vision, Open3DHOI, 3D reconstruction, HOI dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction</title>
  <link rel="icon" type="image/x-icon" href="static/images/fav.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="./slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="./slick/slick-theme.css"/>

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/wenboran2002/" target="_blank">Boran Wen</a><sup>1</sup></sup>,</span>
                <span class="author-block">
                  <a href="https://dingbang777.github.io/" target="_blank">DingBang Huang<sup>1</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://github.com/Amadeus0079" target="_blank"><sup>1</sup></a>,</span>
                    <span class="author-block">
                      <a href="https://github.com/jakecodespace" target="_blank">Jiahong Zhou<sup>1</sup></a>,</span>
                      
                          
                          <a href="https://dirtyharrylyl.github.io/" target="_blank">Yong-Lu Li<sup>1</sup></a>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University, <sup>2</sup>East China Normal University<br>CVPR 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                    <!-- Arxiv PDF link -->
                    <span class="link-block">
                      <a href="#" target="_blank"
                      class="external-link button is-normal is-rounded is-dark animated-button">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                      </a>
                    </span>

                    <!-- Github link -->
                    <span class="link-block">
                      <a href="#" target="_blank"
                      class="external-link button is-normal is-rounded is-dark animated-button">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                      </a>
                    </span>

                    <!-- ArXiv abstract Link -->
                    <span class="link-block">
                      <a href="#" target="_blank"
                      class="external-link button is-normal is-rounded is-dark animated-button">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                      </a>
                    </span>

                    <!-- Gradio Link -->
                    <span class="link-block">
                      <a href="#" target="_blank"
                      class="external-link button is-normal is-rounded is-dark animated-button">
                      <span class="icon">
                        <i class="fas fa-play-circle"></i>
                      </span>
                      <span>Gradio</span>
                      </a>
                    </span>

                    <!-- Dataset Link -->
                    <span class="link-block">
                      <a href="#" target="_blank"
                      class="external-link button is-normal is-rounded is-dark animated-button">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Dataset</span>
                      </a>
                    </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Video -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls width="100%">
        <source src="static/videos/demo.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction.
      </h2>
    </div>
  </div>
</section>
<!-- End Teaser Video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <p>Reconstructing human-object interactions (HOI) from single images is fundamental in computer vision.
            Existing methods are primarily trained and tested on indoor scenes due to the lack of 3D data, particularly constrained by the object variety, making it challenging to generalize to real-world scenes with a wide range of objects. The limitations of previous 3D HOI datasets were primarily due to the difficulty in acquiring 3D object assets. However, with the development of 3D reconstruction from single images, recently it has become possible to reconstruct various objects from 2D HOI images. We therefore propose a pipeline for annotating fine-grained 3D humans, objects, and their interactions from single images. We annotated 2.5k+ 3D HOI assets from existing 2D HOI datasets and built the first open-vocabulary in-the-wild 3D HOI dataset <strong>Open3DHOI</strong>, to serve as a future test set. Moreover, we design a novel Gaussian-HOI optimizer, which efficiently reconstructs the spatial interactions between humans and objects while learning the contact regions. Besides the 3D HOI reconstruction, we also propose several new tasks for 3D HOI understanding to pave the way for future work.</p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- End image carousel -->

  <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-2 has-text-centered">Visualization</h2>
      <div class="columns is-multiline is-centered">
        <div class="column is-one-third">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="static/videos/demo_1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="static/videos/demo_2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="static/videos/demo_3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="static/videos/demo_4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="static/videos/demo_5.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="static/videos/demo_6.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
    <h2 class="title is-2 has-text-centered">3DHOI Annotation</h2>
      <!-- Annotation. -->
      <div id="results-carousel-annot" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/annot2.jpg" alt="MY ALT TEXT" class="centered-image-annot"/>
          <h2 class="subtitle has-text-centered">
            Coarse Reconstruction and Object category distribution in our dataset Open3DHOI. We first obtain depth from the
            images and generate point clouds. Given masks, we extract the
            corresponding point clouds for the person (pink) and object (blue).
            We obtain a rough reconstruction by matching the MESH vertices
            of the person and the object with the depth point cloud.
          </h2>
        </div>

        <div class="item">
          <!-- Your image here -->
          <img src="static/images/annot1.jpg" alt="MY ALT TEXT" class="centered-image-annot"/>
          <h2 class="subtitle has-text-centered">
            Annotation Pipeline. (a) Filtering. Given the reconstructed human and object meshes, annotators assess the quality. If the human
            reconstruction is eligible, the contact area is further annotated. If the object reconstruction fails, the mask is redrawn manually and the
            reconstruction is performed again. (b) Given the 3D human interaction through coarse reconstruction, we adjust the object position in
            Blender. For example, the rough annotation of the couch and the human body shows a mesh collision. We move the object to make sure the
            person is correctly seated on the couch. (c) We use a fine annotation tool to further align the annotated human and object with the image.
          </h2>
        </div>

      </div>

    </div>

  </div>
</section>


  <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-2 has-text-centered">Pipeline</h2>
      <div class="columns is-centered">
        <div class="column">
          <img src="static/images/pipeline.jpg" alt="Pipeline Image" class="centered-image-pipeline"/>
          <h2 class="subtitle has-text-centered">
            Our pipeline. The optimizer first converts the human and object into 3D Gaussian points, then calculates a rendering loss by
comparing the Gaussian-rendered image with the ground truth image. This loss is backpropagated to update the object’s pose parameters
and the human’s LBS parameters. We also calculate an HOI loss, which includes collision, depth and contact losses, the red overlapping
areas between the human and object in the image represent collision regions and the dashed lines represent the ground truth depth and the
depth during the optimization process. Finally, we refine the result by optimizing the contact regions
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-2 has-text-centered">Results</h2>
      <div class="columns is-centered">
        <div class="column">
          <img src="static/images/result.jpg" alt="Pipeline Image" class="centered-image-pipeline"/>

        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!--  <script type="text/javascript" src="./static/js/jquery-1.11.0.min.js"></script>-->
<!--  <script type="text/javascript" src="./static/js/jquery-migrate-1.2.1.min.js"></script>-->
  <script type="text/javascript" src="./slick/slick.min.js"></script>
<!-- Statcounter tracking code -->

<script type="text/javascript">
    $(document).ready(function(){
//   $('.carousel_cl').slick(
//           {
//             dots: true,
//             arrows: true,
//             infinite: true,
//             speed: 300,
//             lazyLoad: 'ondemand',
//             slidesToShow: 3,
//             slidesToScroll: 1
// }
// );
});
    </script>
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
